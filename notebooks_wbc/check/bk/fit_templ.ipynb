{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import sklearn.metrics as m\n",
    "import boost_histogram as bh\n",
    "import glob\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "import boost_histogram as bh\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from cycler import cycler\n",
    "# import mplhep as hep\n",
    "# plt.style.use(hep.style.ROOT)\n",
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "\n",
    "def _p4_from_ptetaphie(pt, eta, phi, energy):\n",
    "    import vector\n",
    "    vector.register_awkward()\n",
    "    return vector.zip({'pt': pt, 'eta': eta, 'phi': phi, 'energy': energy})\n",
    "def _p4_from_ptetaphim(pt, eta, phi, mass):\n",
    "    import vector\n",
    "    vector.register_awkward()\n",
    "    return vector.zip({'pt': pt, 'eta': eta, 'phi': phi, 'mass': mass})\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import reduce\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories\n",
    "from types import SimpleNamespace\n",
    "config = SimpleNamespace(\n",
    "    categories = { # category name: (sample list, selection, label, color)\n",
    "        \"bkg_wjets\": ([\"wjets\"], \"d.lep_pt > 0\", r\"Bkg: W+jets\", \"lightcyan\"),\n",
    "        \"bkg_t_matched\": ([\"ttbarsl\", \"wwsl\", \"twsl\"], \"~d.is_wcb & d.is_t_matched\", r\"Bkg ($t_{bqq'}$)\", \"blue\"),\n",
    "        \"bkg_w_matched\": ([\"ttbarsl\", \"wwsl\", \"twsl\"], \"~d.is_wcb & d.is_w_matched\", r\"Bkg ($W_{qq'}$)\", \"red\"),\n",
    "        \"bkg_tbc_matched\": ([\"ttbarsl\", \"wwsl\", \"twsl\"], \"~d.is_wcb & d.is_tbc_matched\", r\"Bkg ($t_{bc}$)\", \"orange\"),\n",
    "        \"bkg_tbq_matched\": ([\"ttbarsl\", \"wwsl\", \"twsl\"], \"~d.is_wcb & d.is_tbq_matched\", r\"Bkg ($t_{bq'}$)\", \"wheat\"),\n",
    "        \"bkg_non_matched\": ([\"ttbarsl\", \"wwsl\", \"twsl\"], \"~d.is_wcb & d.is_non_matched\", r\"Bkg (non)\", \"lightyellow\"),\n",
    "\n",
    "        # signals\n",
    "        \"sig_t_matched\": ([\"ttbarsl_wcb\", \"wwsl_wcb\", \"twsl_wcb\"], \"d.is_wcb & d.is_t_matched\", r\"Sig ($t_{bqq'}$)\", \"blue\"),\n",
    "        \"sig_w_matched\": ([\"ttbarsl_wcb\", \"wwsl_wcb\", \"twsl_wcb\"], \"d.is_wcb & d.is_w_matched\", r\"Sig ($W_{qq'}$)\", \"red\"),\n",
    "        \"sig_tbc_matched\": ([\"ttbarsl_wcb\", \"wwsl_wcb\", \"twsl_wcb\"], \"d.is_wcb & d.is_tbc_matched\", r\"Sig ($t_{bc}$)\", \"orange\"),\n",
    "        \"sig_tbq_matched\": ([\"ttbarsl_wcb\", \"wwsl_wcb\", \"twsl_wcb\"], \"d.is_wcb & d.is_tbq_matched\", r\"Sig ($t_{bq'}$)\", \"wheat\"),\n",
    "        \"sig_non_matched\": ([\"ttbarsl_wcb\", \"wwsl_wcb\", \"twsl_wcb\"], \"d.is_wcb & d.is_non_matched\", r\"Sig (non)\", \"lightyellow\"),\n",
    "    },\n",
    "\n",
    "    variables = {\n",
    "        # for template making\n",
    "        \"sophon_discr2_dnn_hist2d\": ((\"d.fj_sophon_discr2[:,0]\", \"d.score_is_w_matched\"), (bh.axis.Regular(100, 0.9, 1), bh.axis.Regular(100, 0, 1))),\n",
    "        \"sophon_discr3_dnn_hist2d\": ((\"d.fj_sophon_discr3[:,0]\", \"d.score_is_w_matched\"), (bh.axis.Regular(100, 0.9, 1), bh.axis.Regular(100, 0, 1))),\n",
    "    },\n",
    "\n",
    "    signal_mul_factor = 10,\n",
    "\n",
    "    categories_merged = {\n",
    "        \"bkg_wjets\": [\"bkg_wjets\"],\n",
    "        \"bkg_allwhad_tbc\": [\"bkg_tbc_matched\"],\n",
    "        \"bkg_allwhad_others\": [\"bkg_t_matched\", \"bkg_w_matched\", \"bkg_tbq_matched\", \"bkg_non_matched\"],\n",
    "        \"sig\": [\"sig_t_matched\", \"sig_w_matched\", \"sig_tbc_matched\", \"sig_tbq_matched\", \"sig_non_matched\"],\n",
    "    }\n",
    ")\n",
    "# btag_wp = {\"L\": 0.0557, \"M\": 0.297, \"T\": 0.725}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"pickles/wcb_ana_templ_2dhist_spdiscr2_newsample_v0216.pkl\", \"rb\") as f:\n",
    "    templs_summary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply aux weight scale: 6.666666666666667\n",
      "876.1882682047756 182198.26210505684\n"
     ]
    }
   ],
   "source": [
    "## write templates\n",
    "filedir = './datacards/boosted'\n",
    "aux_weight_scale = 3000. / 450 # for HL-LHC\n",
    "# aux_weight_scale = 1.\n",
    "aux_flavor_strength = 1\n",
    "\n",
    "# 0.xxx, 0.xx\n",
    "sophon_discr_thres = 0.95; evt_dnn_thres = 0.87  # 0.951, 0.87 optimal for strength=1; 0.95, 0.86 optimal for strength=0.5\n",
    "\n",
    "########################################################\n",
    "import copy\n",
    "_templs = copy.deepcopy(templs_summary) ## important!\n",
    "\n",
    "hist_out = {}\n",
    "# should first sum over the first Sophon discr index (start:end:bh.sum)\n",
    "for cm in config.categories_merged:\n",
    "    hist_out[cm] = _templs[\"nom\"][cm][int(sophon_discr_thres*1000)-900::bh.sum, :] ## important!\n",
    "\n",
    "for target_flv in [\"bjet\", \"cjet\", \"ljet\"]:\n",
    "    for target_region in [\"B1\", \"B2\", \"C1\", \"C2\", \"N\"]:\n",
    "        for target_variation in [\"up\", \"down\"]:\n",
    "            _variation = \"Up\" if target_variation == \"up\" else \"Down\"\n",
    "            # get the inclusive non-flavour-cut template's total yield\n",
    "            nevt_tot_nom = sum(_templs[\"nom\"][cm].view(flow=True).value.sum() for cm in config.categories_merged)\n",
    "            nevt_tot_flv = sum(_templs[f\"{target_flv}_{target_region}_{target_variation}\"][cm].view(flow=True).value.sum() for cm in config.categories_merged)\n",
    "            fac =  nevt_tot_nom / nevt_tot_flv\n",
    "\n",
    "            for cm in config.categories_merged:\n",
    "                n = f\"{cm}_ftag_{target_flv}_{target_region}{_variation}\"\n",
    "                hist_out[n] = _templs[f\"{target_flv}_{target_region}_{target_variation}\"][cm][int(sophon_discr_thres*1000)-900::bh.sum, :] ## important!\n",
    "                # make sure the total yields doesn't change\n",
    "                hist_out[n].view().value *= fac\n",
    "                hist_out[n].view().variance *= fac**2\n",
    "                # hist_out[n].view().value *= sum(hist_out[cm].view().value) / sum(hist_out[n].view().value)\n",
    "                # hist_out[n].view().variance *= (sum(hist_out[cm].view().value) / sum(hist_out[n].view().value))**2\n",
    "\n",
    "hist_out['data_obs'] = sum(hist_out[cm] for cm in config.categories_merged)\n",
    "\n",
    "# aux weight scale\n",
    "for n in hist_out:\n",
    "    hist_out[n].view().value *= aux_weight_scale\n",
    "    hist_out[n].view().variance *= aux_weight_scale**2\n",
    "\n",
    "\n",
    "# rebin the DNN score\n",
    "# === old setup ===\n",
    "# for n in hist_out:\n",
    "#     # hist_out[n] = hist_out[n][bh.loc(0.96):bh.loc(1.0):bh.rebin(4)] # 1 bin from 0.96--1\n",
    "#     hist_out[n] = hist_out[n][bh.loc(0.6):bh.loc(1.0):bh.rebin(4)]\n",
    "#     hist_out[n].view().variance *= 0\n",
    "# ======\n",
    "def rebin_hist_var_width(orig_hist, new_axis):\n",
    "    new_hist = bh.Histogram(new_axis, storage=bh.storage.Weight())\n",
    "    for i in range(orig_hist.axes[0].size):\n",
    "        bin_center = orig_hist.axes[0].centers[i]\n",
    "        if bin_center > new_axis.edges[0] and bin_center < new_axis.edges[-1]:\n",
    "            new_idx = np.searchsorted(new_axis.edges, bin_center) - 1\n",
    "            new_hist.view().value[new_idx] += orig_hist.view().value[i]\n",
    "            new_hist.view().variance[new_idx] += orig_hist.view().variance[i]\n",
    "    return new_hist\n",
    "\n",
    "# write two templates: SR1 and SR2\n",
    "hist_out1, hist_out2 = {}, {}\n",
    "for n in hist_out:\n",
    "    hist_out1[n] = rebin_hist_var_width(hist_out[n], bh.axis.Variable([evt_dnn_thres, 1.0])) # pass DNN bin\n",
    "    hist_out1[n].view().value = np.maximum(hist_out1[n].view().value, 1e-3)\n",
    "    hist_out1[n].view().variance *= 0\n",
    "\n",
    "    hist_out2[n] = rebin_hist_var_width(hist_out[n], bh.axis.Variable([0, evt_dnn_thres])) # fail DNN bin\n",
    "    hist_out2[n].view().value = np.maximum(hist_out2[n].view().value, 1e-3)\n",
    "    hist_out2[n].view().variance *= 0\n",
    "\n",
    "import re\n",
    "for n in hist_out:\n",
    "    if 'ftag' in n:\n",
    "        hist_out1[n].view().value = hist_out1[n].view().value * aux_flavor_strength + hist_out1[re.sub(r\"(_ftag_.*)\", \"\", n)].view().value * (1 - aux_flavor_strength)\n",
    "        hist_out2[n].view().value = hist_out2[n].view().value * aux_flavor_strength + hist_out2[re.sub(r\"(_ftag_.*)\", \"\", n)].view().value * (1 - aux_flavor_strength)\n",
    "\n",
    "print('Apply aux weight scale:', aux_weight_scale)\n",
    "print(hist_out1['data_obs'].values().sum(), hist_out2['data_obs'].values().sum())\n",
    "\n",
    "with uproot.recreate(f'{filedir}/input_SR1.root') as fw:\n",
    "    for n in hist_out1:\n",
    "        fw[n] = hist_out1[n]\n",
    "with uproot.recreate(f'{filedir}/input_SR2.root') as fw:\n",
    "    for n in hist_out2:\n",
    "        fw[n] = hist_out2[n]\n",
    "\n",
    "lines = open(f'{filedir}/datacard.txt').readlines()\n",
    "for i, line in enumerate(lines):\n",
    "    if line.startswith('observation'):\n",
    "        lines[i] = f'observation {hist_out1[\"data_obs\"].values().sum()} {hist_out2[\"data_obs\"].values().sum()}\\n'\n",
    "with open(f'{filedir}/datacard.txt', 'w') as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories\n",
    "from types import SimpleNamespace\n",
    "config = SimpleNamespace(\n",
    "    categories = { # category name: (sample list, selection, label, color)\n",
    "        \"bkg_wjets\": ([\"wjets\"], \"d.lep_pt > 0\", r\"Bkg: W+jets\", \"lightcyan\"),\n",
    "        \"bkg_ttbarsl\": ([\"ttbarsl\"], \"~d.is_wcb\", r\"Bkg ($t\\overline{t}$)\", \"orange\"),\n",
    "        \"bkg_twsl\": ([\"twsl\"], \"~d.is_wcb\", r\"Bkg ($tW$)\", \"magenta\"),\n",
    "        \"bkg_wwsl\": ([\"wwsl\"], \"~d.is_wcb\", r\"Bkg (WW)\", \"green\"),\n",
    "\n",
    "        \"sig_ttbarsl\": ([\"ttbarsl_wcb\"], \"d.is_wcb\", r\"Sig ($t\\overline{t}$)\", \"orange\"),\n",
    "        \"sig_twsl\": ([\"twsl_wcb\"], \"d.is_wcb\", r\"Sig ($tW$)\", \"magenta\"),\n",
    "        \"sig_wwsl\": ([\"wwsl_wcb\"], \"d.is_wcb\", r\"Sig (WW)\", \"green\"),\n",
    "    },\n",
    "\n",
    "    variables = {\n",
    "        \"evt_dnn_score_0p9_1_1000\": (\"d.evt_dnn_score\", bh.axis.Regular(1000, 0.9, 1)), # to make templates\n",
    "    },\n",
    "\n",
    "    signal_mul_factor = 10,\n",
    "\n",
    "    categories_merged = {\n",
    "        \"bkg_wjets\": [\"bkg_wjets\"],\n",
    "        \"bkg_allwhad\": [\"bkg_ttbarsl\", \"bkg_twsl\", \"bkg_wwsl\"],\n",
    "        \"sig\": [\"sig_ttbarsl\", \"sig_twsl\", \"sig_wwsl\"],\n",
    "    },\n",
    ")\n",
    "# btag_wp = {\"L\": 0.0557, \"M\": 0.297, \"T\": 0.725}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open(\"pickles/wcb_rsvana_templ_newsample_v0216_nbc3.pkl\", \"rb\") as f:\n",
    "#     templs_summary = pickle.load(f)\n",
    "# to exclude fatjet phase space from resolved channel, use this one\n",
    "with open(\"pickles/wcb_rsvana_templ_newsample_v0216_nbc3_excludefj.pkl\", \"rb\") as f:\n",
    "    templs_summary = pickle.load(f)\n",
    "\n",
    "with open(\"pickles/wcb_rsvana_templ_newsample_v0216.pkl\", \"rb\") as f:\n",
    "    templs_summary_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply aux weight scale: 6.666666666666667\n",
      "348.50599602581934\n"
     ]
    }
   ],
   "source": [
    "## new impl. for templs adding a flavour tagger cut\n",
    "\n",
    "## write templates\n",
    "filedir = './datacards/resolved'\n",
    "aux_weight_scale = 3000. / 450 # for HL-LHC\n",
    "aux_flavor_strength = 1. *1/np.sqrt(3000/140)\n",
    "# aux_flavor_strength = 0.5\n",
    "\n",
    "# 0.xxx, 0.xx\n",
    "evt_dnn_thres = 0.990  # 0.990-0.992 all optimal..; for stregth=0.5, 0.986; for stregth=0.2, 0.983\n",
    "\n",
    "########################################################\n",
    "import copy\n",
    "_templs = copy.deepcopy(templs_summary) ## important!\n",
    "_templs0 = copy.deepcopy(templs_summary_orig) ## important!\n",
    "\n",
    "hist_out = {}\n",
    "for cm in config.categories_merged:\n",
    "    hist_out[cm] = _templs[\"nom\"][cm]\n",
    "\n",
    "for target_flv in [\"bjet\", \"cjet\", \"ljet\"]:\n",
    "    for target_region in [\"B1\", \"B2\", \"C1\", \"C2\", \"N\"]:\n",
    "        for target_variation in [\"up\", \"down\"]:\n",
    "            _variation = \"Up\" if target_variation == \"up\" else \"Down\"\n",
    "            # get the inclusive non-flavour-cut template's total yield\n",
    "            nevt_tot_nom = sum(_templs0[\"nom\"][cm].view(flow=True).value.sum() for cm in config.categories_merged)\n",
    "            nevt_tot_flv = sum(_templs0[f\"{target_flv}_{target_region}_{target_variation}\"][cm].view(flow=True).value.sum() for cm in config.categories_merged)\n",
    "            fac =  nevt_tot_nom / nevt_tot_flv\n",
    "\n",
    "            for cm in config.categories_merged:\n",
    "                n = f\"{cm}_ftag_{target_flv}_{target_region}{_variation}\"\n",
    "                hist_out[n] = _templs[f\"{target_flv}_{target_region}_{target_variation}\"][cm]\n",
    "                # make sure the total yields doesn't change (use non-flavour-cut template)\n",
    "                hist_out[n].view().value *= fac\n",
    "                hist_out[n].view().variance *= fac**2\n",
    "\n",
    "hist_out['data_obs'] = sum(hist_out[cm] for cm in config.categories_merged)\n",
    "\n",
    "# aux weight scale\n",
    "for n in hist_out:\n",
    "    hist_out[n].view().value *= aux_weight_scale\n",
    "    hist_out[n].view().variance *= aux_weight_scale**2\n",
    "\n",
    "# rebin the DNN score\n",
    "# === old setup ===\n",
    "# for n in hist_out:\n",
    "#     # hist_out[n] = hist_out[n][bh.loc(0.96):bh.loc(1.0):bh.rebin(4)] # 1 bin from 0.96--1\n",
    "#     hist_out[n] = hist_out[n][bh.loc(0.6):bh.loc(1.0):bh.rebin(4)]\n",
    "#     hist_out[n].view().variance *= 0\n",
    "# ======\n",
    "def rebin_hist_var_width(orig_hist, new_axis):\n",
    "    new_hist = bh.Histogram(new_axis, storage=bh.storage.Weight())\n",
    "    for i in range(orig_hist.axes[0].size):\n",
    "        bin_center = orig_hist.axes[0].centers[i]\n",
    "        if bin_center > new_axis.edges[0] and bin_center < new_axis.edges[-1]:\n",
    "            new_idx = np.searchsorted(new_axis.edges, bin_center) - 1\n",
    "            new_hist.view().value[new_idx] += orig_hist.view().value[i]\n",
    "            new_hist.view().variance[new_idx] += orig_hist.view().variance[i]\n",
    "    return new_hist\n",
    "\n",
    "for n in hist_out:\n",
    "    hist_out[n] = rebin_hist_var_width(hist_out[n], bh.axis.Variable([evt_dnn_thres, 1]))\n",
    "    hist_out[n].view().value = np.maximum(hist_out[n].view().value, 1e-3)\n",
    "    hist_out[n].view().variance *= 0\n",
    "\n",
    "import re\n",
    "for n in hist_out:\n",
    "    if 'ftag' in n:\n",
    "        hist_out[n].view().value = hist_out[n].view().value * aux_flavor_strength + hist_out[re.sub(r\"(_ftag_.*)\", \"\", n)].view().value * (1 - aux_flavor_strength)\n",
    "\n",
    "print('Apply aux weight scale:', aux_weight_scale)\n",
    "print(hist_out['data_obs'].values().sum())\n",
    "\n",
    "with uproot.recreate(f'{filedir}/input_SR.root') as fw:\n",
    "    for n in hist_out:\n",
    "        fw[n] = hist_out[n]\n",
    "\n",
    "lines = open(f'{filedir}/datacard.txt').readlines()\n",
    "for i, line in enumerate(lines):\n",
    "    if line.startswith('observation'):\n",
    "        lines[i] = f'observation {hist_out[\"data_obs\"].values().sum()}\\n'\n",
    "with open(f'{filedir}/datacard.txt', 'w') as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weaver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
